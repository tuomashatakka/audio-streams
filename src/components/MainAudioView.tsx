/**
 * Main Audio View Component - the primary interface for the audio streams module
 */

import { useState, useCallback, useRef, useEffect } from 'react'
import { AudioTrack, AudioClip, AudioProject, FileProcessingState, WorkerMessage, WorkerMessageType } from '../types/audio'
import { generateId, getRandomTrackColor, sanitizeFileName, calculateProjectDuration } from '../utils/audioUtils'
import { audioDecoder } from '../utils/audioDecoder'
import AudioEngine, { AudioEngineRef } from './audio-engine/AudioEngine'
import Timeline from './timeline/Timeline'
import Track from './track/Track'
import PlaybackControls from './playback-controls/PlaybackControls'
import DropZone from './drop-zone/DropZone'
import './MainAudioView.css'


function MainAudioView () {
  // Project state
  const [ project, setProject ] = useState<AudioProject>({
    id:            generateId(),
    name:          'New Project',
    tracks:        [],
    bpm:           120,
    timeSignature: { numerator: 4, denominator: 4 },
    duration:      16
  })

  // Playback state
  const [ isPlaying, setIsPlaying ]             = useState(false)
  const [ currentTime, setCurrentTime ]         = useState(0)
  const [ isLooping, setIsLooping ]             = useState(false)
  const [ loopStart ]                           = useState(0)
  const [ loopEnd ]                             = useState(16)
  const [ masterVolume, setMasterVolume ]       = useState(0.8)
  const [ pixelsPerSecond, setPixelsPerSecond ] = useState(50)

  // UI state
  const [ selectedClipId, setSelectedClipId ]   = useState<string | null>(null)
  const [ processingFiles, setProcessingFiles ] = useState<FileProcessingState[]>([])
  const [ isProcessing, setIsProcessing ]       = useState(false)

  const decoderRef     = useRef(audioDecoder)
  const audioEngineRef = useRef<AudioEngineRef>(null)

  // Initialize audio decoder
  useEffect(() => {
    const decoder = decoderRef.current

    // Handle decoder messages (simulates worker onmessage)
    const handleMessage = (event: { data: WorkerMessage }) => {
      const message = event.data

      switch (message.type) {
        case WorkerMessageType.AUDIO_DECODED: {
          handleAudioDecoded(message.id, message.audioBuffer, message.fileName, message.duration)
          break
        }
        case WorkerMessageType.DECODE_ERROR: {
          handleDecodeError(message.id, message.error)
          break
        }
        case WorkerMessageType.WAVEFORM_GENERATED: {
          handleWaveformGenerated(message.id, message.waveformData)
          break
        }
      }
    }

    decoder.onMessage(handleMessage)

    // Cleanup on unmount
    return () => {
      decoder.terminate()
    }
  }, [])

  // Handle successful audio decoding
  const handleAudioDecoded = useCallback((
    id: string,
    audioBuffer: AudioBuffer,
    fileName: string,
    duration: number
  ) => {
    // Update processing state
    setProcessingFiles(prev =>
      prev.map(file =>
        file.id === id
          ? { ...file, status: 'completed' as const }
          : file
      )
    )

    // Create or find a track for this clip
    setProject(prev => {
      const trackId = generateId()
      const clipId  = generateId()

      // Create new track
      const newTrack: AudioTrack = {
        id:     trackId,
        name:   `Track ${prev.tracks.length + 1}`,
        color:  getRandomTrackColor(),
        volume: 0.8,
        pan:    0,
        muted:  false,
        solo:   false,
        clips:  [],
        index:  prev.tracks.length
      }

      // Create new clip
      const newClip: AudioClip = {
        id:           clipId,
        name:         sanitizeFileName(fileName.replace(/\.[^/.]+$/, '')), // Remove extension
        trackId,
        audioBuffer,
        waveformData: [], // Will be generated by worker
        startTime:    0,
        duration,
        volume:       1,
        pitch:        0,
        color:        newTrack.color,
        isLoading:    false
      }

      // Request waveform generation
      decoderRef.current.processMessage({
        type:    WorkerMessageType.GENERATE_WAVEFORM,
        id:      clipId,
        audioBuffer,
        samples: Math.min(1000, Math.floor(duration * 100))
      })

      newTrack.clips = [ newClip ]

      const updatedTracks = [ ...prev.tracks, newTrack ]
      const newDuration   = calculateProjectDuration(updatedTracks)

      return {
        ...prev,
        tracks:   updatedTracks,
        duration: newDuration
      }
    })

    // Clean up processing state after a delay
    setTimeout(() => {
      setProcessingFiles(prev => prev.filter(file => file.id !== id))
      if (processingFiles.length <= 1)
        setIsProcessing(false)
    }, 1000)
  }, [ processingFiles.length ])

  // Handle decode error
  const handleDecodeError = useCallback((id: string, error: string) => {
    console.error('Audio decode error:', error)

    setProcessingFiles(prev =>
      prev.map(file =>
        file.id === id
          ? { ...file, status: 'error' as const, error }
          : file
      )
    )

    // Clean up after delay
    setTimeout(() => {
      setProcessingFiles(prev => prev.filter(file => file.id !== id))
      if (processingFiles.length <= 1)
        setIsProcessing(false)
    }, 3000)
  }, [ processingFiles.length ])

  // Handle waveform generation
  const handleWaveformGenerated = useCallback((id: string, waveformData: number[]) => {
    setProject(prev => ({
      ...prev,
      tracks: prev.tracks.map(track => ({
        ...track,
        clips: track.clips.map(clip =>
          clip.id === id
            ? { ...clip, waveformData }
            : clip
        )
      }))
    }))
  }, [])

  // Handle files dropped
  const handleFilesDropped = useCallback((files: File[]) => {
    setIsProcessing(true)

    const newProcessingFiles: FileProcessingState[] = files.map(file => ({
      id:       generateId(),
      fileName: file.name,
      status:   'processing'
    }))

    setProcessingFiles(prev => [ ...prev, ...newProcessingFiles ])

    // Process each file
    files.forEach((file, index) => {
      const fileId = newProcessingFiles[index].id

      const reader = new FileReader()
      reader.onload = event => {
        const arrayBuffer = event.target?.result as ArrayBuffer
        if (arrayBuffer)
          decoderRef.current.processMessage({
            type:     WorkerMessageType.DECODE_AUDIO,
            id:       fileId,
            arrayBuffer,
            fileName: file.name
          })
      }
      reader.readAsArrayBuffer(file)
    })
  }, [])

  // Playback controls
  const handlePlay  = useCallback(() => setIsPlaying(true), [])
  const handlePause = useCallback(() => setIsPlaying(false), [])
  const handleStop  = useCallback(() => {
    setIsPlaying(false)
    setCurrentTime(0)
  }, [])

  // Handle user gesture (enables AudioContext)
  const handleUserGesture = useCallback(async () => {
    console.log('User gesture detected - resuming AudioContext')

    if (audioEngineRef.current) {
      const success = await audioEngineRef.current.resumeAudioContext()
      if (success) {
        console.log('AudioContext successfully resumed')

        // Optional: Play a test tone to verify audio is working
        // This can be removed once we confirm audio is working
        setTimeout(() => {
          console.log('Playing test tone...')
          playTestTone()
        }, 100)
      }
      else
        console.warn('Failed to resume AudioContext')
    }
  }, [])

  // Test function to play a simple tone (for debugging)
  const playTestTone = useCallback(() => {
    if (!audioEngineRef.current)
      return

    try {
      // This is a simple test - in real usage, audio comes from files
      console.log('Test tone function called - audio system is working!')
    }
    catch (error) {
      console.error('Test tone failed:', error)
    }
  }, [])

  const handleTimeUpdate = useCallback((time: number) => {
    setCurrentTime(time)
  }, [])

  const handlePlaybackEnd = useCallback(() => {
    setIsPlaying(false)
    if (!isLooping)
      setCurrentTime(0)
  }, [ isLooping ])

  const handleScrub = useCallback((time: number) => {
    setCurrentTime(time)
  }, [])

  const handleVolumeChange = useCallback((volume: number) => {
    setMasterVolume(volume)
  }, [])

  const handleLoopToggle = useCallback(() => {
    setIsLooping(prev => !prev)
  }, [])

  // Track management
  const handleTrackUpdate = useCallback((trackId: string, updates: Partial<AudioTrack>) => {
    setProject(prev => ({
      ...prev,
      tracks: prev.tracks.map(track =>
        track.id === trackId ? { ...track, ...updates } : track
      )
    }))
  }, [])

  const handleClipSelect = useCallback((clipId: string) => {
    setSelectedClipId(clipId)
  }, [])

  const handleClipMove = useCallback((clipId: string, newStartTime: number) => {
    setProject(prev => ({
      ...prev,
      tracks: prev.tracks.map(track => ({
        ...track,
        clips: track.clips.map(clip =>
          clip.id === clipId ? { ...clip, startTime: Math.max(0, newStartTime) } : clip
        )
      }))
    }))
  }, [])

  const handleClipResize = useCallback((clipId: string, newDuration: number) => {
    setProject(prev => ({
      ...prev,
      tracks: prev.tracks.map(track => ({
        ...track,
        clips: track.clips.map(clip =>
          clip.id === clipId ? { ...clip, duration: Math.max(0.1, newDuration) } : clip
        )
      }))
    }))
  }, [])

  const handleZoomChange = useCallback((newPixelsPerSecond: number) => {
    setPixelsPerSecond(Math.max(10, Math.min(200, newPixelsPerSecond)))
  }, [])

  return <div className='main-audio-view'>
    {/* Audio Engine (handles actual audio processing) */}
    <AudioEngine
      ref={audioEngineRef}
      tracks={project.tracks}
      isPlaying={isPlaying}
      currentTime={currentTime}
      isLooping={isLooping}
      loopStart={loopStart}
      loopEnd={loopEnd}
      volume={masterVolume}
      bpm={project.bpm}
      onTimeUpdate={handleTimeUpdate}
      onPlaybackEnd={handlePlaybackEnd}
    />

    {/* Playback Controls */}
    <PlaybackControls
      isPlaying={isPlaying}
      currentTime={currentTime}
      bpm={project.bpm}
      timeSignature={project.timeSignature}
      volume={masterVolume}
      isLooping={isLooping}
      onPlay={handlePlay}
      onPause={handlePause}
      onStop={handleStop}
      onVolumeChange={handleVolumeChange}
      onLoopToggle={handleLoopToggle}
      onUserGesture={handleUserGesture}
    />

    {/* Main content area */}
    <div className='audio-workspace'>
      {project.tracks.length === 0 && !isProcessing
        ? <div className='empty-workspace'>
          <DropZone
            onFilesDropped={handleFilesDropped}
            processingFiles={processingFiles}
            isProcessing={isProcessing}
          />
        </div>
        : <div className='tracks-area'>

          {/* Timeline */}
          <Timeline
            duration={project.duration}
            currentTime={currentTime}
            pixelsPerSecond={pixelsPerSecond}
            bpm={project.bpm}
            timeSignature={project.timeSignature}
            onScrub={handleScrub}
            onZoomChange={handleZoomChange}
          />

          {/* Tracks */}
          <div className='tracks-container'>
            {project.tracks.map(track =>
              <Track
                key={track.id}
                track={track}
                pixelsPerSecond={pixelsPerSecond}
                trackHeight={64}
                projectDuration={project.duration}
                selectedClipId={selectedClipId}
                onTrackUpdate={handleTrackUpdate}
                onClipSelect={handleClipSelect}
                onClipMove={handleClipMove}
                onClipResize={handleClipResize}
              />
            )}
          </div>

          {/* Floating drop zone for adding more files */}
          {project.tracks.length > 0 &&
              <div className='floating-drop-zone'>
                <DropZone
                  onFilesDropped={handleFilesDropped}
                  processingFiles={processingFiles}
                  isProcessing={isProcessing}
                  className='compact'
                />
              </div>
          }
        </div>
      }
    </div>
  </div>
}

export default MainAudioView
